{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sanford-Lab/satellite_cnns/blob/main/benin_predictions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook uses a dataset of Benin to train a model to predict which pixels belong to a village. The dataset takes in the feature collection from 'projects/satellite-cnns/assets/benin/voronoi_villages' which has circles around villages in Benin and uses this as a target for prediction. As input, the landsat 7 imagery of Benin is used.\n",
        "\n",
        "The notebook has the sections:\n",
        "\n",
        "1.   Setup & General settings\n",
        "2.   Read data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTES:**\n",
        "\n",
        "*   The Benin prediction problem is a segmentation task.\n",
        "\n",
        "\n",
        "> YB: Classification: Currently, the model predicts a pixel-wise classification. We wondered whether this is really necessary since our ground truth is captured in larger patches. Therefore, it might be sufficient to predict one value for a rectangle and take care of the border areas by combining the values of all relevant rectangles. If we use pixel-wise predictions, the model might simply learn how to draw borders. Could you explain why pixel-wise classification makes sense in the case of Benin? Maybe we are overlooking an important aspect also for the Kenyan case…\n",
        "\n",
        "> DD: [... F]or Benin we need it because we need to both predict treatment status and outcomes, and use residuals from both of those models. For each even a relatively small tile size is much larger than the spatial scale of interest.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kfqAZE7xmkZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setup & General settings"
      ],
      "metadata": {
        "id": "94QK42W6Qx11"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "## 1.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh4fElJHEplh",
        "outputId": "296e5466-ac42-40f1-fad7-47db9cef4ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/satellite_cnns': No such file or directory\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%rm -r /content/satellite_cnns\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AHIHF4D6FORr"
      },
      "outputs": [],
      "source": [
        "branch = \"main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as3xyWjhClJy"
      },
      "outputs": [],
      "source": [
        "# Clone from SPIRES Repo\n",
        "!git clone --branch {branch} https://github.com/Sanford-Lab/satellite_cnns.git\n",
        "%cd /content/satellite_cnns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZIDEamO_gM2"
      },
      "source": [
        "**New**: The notebook now sources files from the Sanford-Lab/satellite_cnns repo. For this notebook, it will use the `benin-data` package. It's built it based on the weather-forcasting notebook patterns and to allow importing for project-specific packages to *plug and play* for data creation. The new patterns should allow the workflow to be much more modular. All a new project would need to do is define 3 main functions `get_inputs_image`, `get_labels_image`, and `sample_points`. The create_dataset.py script should then be able to synthesize the dataset through abstraction. I've kept most of the demonstration functionality the same to show how using the package works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB6CSs7CNaNw"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet --upgrade pip\n",
        "\n",
        "# We need `build` and `virtualenv` to build the local packages.\n",
        "!pip install --quiet build virtualenv\n",
        "\n",
        "# Install Apache Beam and the `benin-data` local package.\n",
        "!pip install apache-beam[gcp] src/benin-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VebxlGd3Jf8Z"
      },
      "outputs": [],
      "source": [
        "# run to manually restart runtime by ending process\n",
        "# exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 General settings"
      ],
      "metadata": {
        "id": "yE9Bod1wQfQJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMU82mRYNfFp"
      },
      "source": [
        "At this point runtime is restarted. Navigate back to our working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54QJoqywApQO",
        "outputId": "a1e4abb6-316b-4164-b00b-35e3969bfd1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/satellite_cnns\n"
          ]
        }
      ],
      "source": [
        "%cd /content/satellite_cnns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2iFfceKGhMC",
        "outputId": "f1079662-43f5-47a2-f2e6-e55246064dab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "#@title Project settings\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Please fill in these values.\n",
        "project = \"satellite-cnns\" #@param {type:\"string\"}\n",
        "bucket = \"beninbucket\" #@param {type:\"string\"}\n",
        "location = \"us-central1\" #@param {type:\"string\"}\n",
        "\n",
        "# Quick input validations.\n",
        "assert project, \"⚠️ Please provide a Google Cloud project ID\"\n",
        "assert bucket, \"⚠️ Please provide a Cloud Storage bucket name\"\n",
        "assert not bucket.startswith('gs://'), f\"⚠️ Please remove the gs:// prefix from the bucket name: {bucket}\"\n",
        "assert location, \"⚠️ Please provide a Google Cloud location\"\n",
        "\n",
        "# Authenticate to Colab.\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set GOOGLE_CLOUD_PROJECT for google.auth.default().\n",
        "os.environ['GOOGLE_CLOUD_PROJECT'] = project\n",
        "\n",
        "# Set the gcloud project for other gcloud commands.\n",
        "!gcloud config set project {project}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y_MaPEZ4HIwr"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import google.auth\n",
        "\n",
        "credentials, _ = google.auth.default()\n",
        "ee.Initialize(\n",
        "    credentials.with_quota_project(None),\n",
        "    project=project,\n",
        "    opt_url=\"https://earthengine-highvolume.googleapis.com\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuHHM5pXQ_DJ"
      },
      "source": [
        "# 2. Read data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Google Cloud Storage"
      ],
      "metadata": {
        "id": "l0OuoVlISe8A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q-e2eD7hHXT"
      },
      "source": [
        "Let's check Google Cloud Storage to see the files in GC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfw7_T1UhSoC"
      },
      "outputs": [],
      "source": [
        "bucket = 'beninbucket'\n",
        "folder = 'yb_test'\n",
        "data_path=f\"gs://{bucket}/{folder}/data\"\n",
        "print(data_path)\n",
        "!gsutil ls -lh {data_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpV-2AohhZeb"
      },
      "source": [
        "Next, let's copy the files to a local directory to look at them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blD6bUqtYfH3"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data-training\n",
        "!gsutil -m cp {data_path}/* data-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LmkHlD-vJ58d"
      },
      "outputs": [],
      "source": [
        "# Use this to wipe the folder if needed\n",
        "#%rm -r data-training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rwTshMtruzG"
      },
      "source": [
        "## 2.2 Look at dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Tdv1we6qeBI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from read_data import DatasetFromPath, test_train_split\n",
        "\n",
        "dataset = DatasetFromPath('data-training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyk2Cv15hvcG"
      },
      "source": [
        "**Note**: to avoid using [Hugging Face 🤗 Datasets](https://huggingface.co/docs/datasets/main/en/index) (like in the weather forcasting sample), we're going to use a custom subclass of PyTorch's `torch.utils.data.Dataset` (`DatsetFromPath`). Hugging Face is nice to use a high-level interface for using datasets and should maybe be implemented in the future, but as of this writing (7/4/2023), VertexAI (what we're using for cloud training) has an issue with it's Hugging Face Trainer API (see [weather sample issue](https://github.com/GoogleCloudPlatform/python-docs-samples/issues/9272))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I2Y9u71sEgT"
      },
      "source": [
        "### Visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbmjRaT4j6JB"
      },
      "source": [
        "Let's grab the dataset (`data`) from our path (`/content/data_training`) and pull the top element from the dataset as `example`. In `DatasetFromPath`, the custom getter utilizes dictionary keys of `inputs` and `labels`, so to grab the inputs of example, we use `example['inputs']`.\n",
        "\n",
        "Check what was run through the pipeline. You should expect to see:\n",
        "- Dataset size of 2 * `POINTS_PER_CLASS`\n",
        "- inputs size of (`PATCH_SIZE`, `PATCH_SIZE`, number of input bands)\n",
        "- labels size of (`PATCH_SIZE`, `PATCH_SIZE`, number of label bands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRF7ypL_BEqf"
      },
      "outputs": [],
      "source": [
        "print(f\"Dataset size: {len(dataset)}\")\n",
        "example = dataset[0]  # random access the first element\n",
        "\n",
        "print(f\"inputs: {example['inputs'].shape}\")\n",
        "print(f\"labels: {example['labels'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_OAWndLkvpa"
      },
      "source": [
        "The `DatsetFromPath` class also allows you to retrieve all of the inputs/labels in a dataset in their raw NumPy array form by indexing with \"inputs\" or \"labels\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hS7A25EEku3W"
      },
      "outputs": [],
      "source": [
        "inputs = dataset['inputs']\n",
        "labels = dataset['labels']\n",
        "\n",
        "print(f\"All inputs: {inputs.shape}\")\n",
        "print(f\"All labels: {labels.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nbS_jurpKO6"
      },
      "source": [
        "Let's view our example using our visualization functionality. For Benin, blue = inside village, red = outside village."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JzlTSkhpMEU"
      },
      "outputs": [],
      "source": [
        "from visualize import show_patch\n",
        "\n",
        "\n",
        "inputs = example['inputs']\n",
        "labels = example['labels']\n",
        "\n",
        "show_patch(inputs, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDir8YB7l2pn"
      },
      "source": [
        "How let's split the dataset into a train and test subset using the test_train_split function. Test differt ratios to see how the dataset splits and view the first of each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX2nnRlrkWxG"
      },
      "outputs": [],
      "source": [
        "TEST_TRAIN_RATIO = 0.8\n",
        "train, test = test_train_split(dataset, ratio=TEST_TRAIN_RATIO)\n",
        "\n",
        "print(f\"Train size: {len(train)}\")\n",
        "train_example = train[0]  # random access the first element\n",
        "print(f\"inputs: {train_example['inputs'].shape}\")\n",
        "print(f\"labels: {train_example['labels'].shape}\")\n",
        "print(f\"Test size: {len(test)}\")\n",
        "test_example = test[0]  # random access the first element\n",
        "print(f\"inputs: {test_example['inputs'].shape}\")\n",
        "print(f\"labels: {test_example['labels'].shape}\\n\")\n",
        "\n",
        "train_inputs = train_example['inputs']\n",
        "train_labels = train_example['labels']\n",
        "test_inputs = test_example['inputs']\n",
        "test_labels = test_example['labels']\n",
        "print(f'Train[0]:')\n",
        "show_patch(train_inputs, train_labels)\n",
        "print(f'Test[0]:')\n",
        "show_patch(test_inputs, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Helper functions"
      ],
      "metadata": {
        "id": "9eUN8VwBEeC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Transformers"
      ],
      "metadata": {
        "id": "x-Z1FLShEula"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_normalize(dataset):\n",
        "  # Access inputs in dataset\n",
        "  inputs = dataset['inputs']\n",
        "\n",
        "  # Reshape to (num_inputs * num_pixels, num_bands)\n",
        "  reshaped_inputs = inputs.reshape(-1, inputs.shape[-1])\n",
        "\n",
        "  # Calculate mean, std and max for each band across all inputs\n",
        "  means = reshaped_inputs.mean(axis=0)\n",
        "  stds = reshaped_inputs.std(axis=0)\n",
        "\n",
        "  return {'mean': means.tolist(), 'std': stds.tolist()}\n",
        "\n",
        "NORM_TRAIN = prep_normalize(train)\n",
        "print(\"Train:\", NORM_TRAIN)\n",
        "NORM_TEST = prep_normalize(test)\n",
        "print(\"Test:\", NORM_TEST)"
      ],
      "metadata": {
        "id": "0T6wuODLqBvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2\n",
        "\n",
        "# Apply image augmention and adjust labels (e.g. after flipping image)\n",
        "\n",
        "# Specify transforms for training\n",
        "transform_train = v2.Compose([\n",
        "  v2.RandomHorizontalFlip(),  # default value is p=0.5\n",
        "  v2.RandomVerticalFlip(),    # default value is p=0.5\n",
        "  v2.Normalize(\n",
        "      mean=NORM_TRAIN['mean'],\n",
        "      std=NORM_TRAIN['std']),\n",
        "  v2.ToTensor()\n",
        "  ])\n",
        "\n",
        "# Specify transforms for testing\n",
        "transform_test = v2.Compose([\n",
        "  v2.ToTensor(),\n",
        "  v2.Normalize(\n",
        "      mean=NORM_TEST['mean'],\n",
        "      std=NORM_TEST['std'])\n",
        "  ])"
      ],
      "metadata": {
        "id": "2qlTpKoEEXLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "VS1cHL9KiBI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "def get_loaders(\n",
        "    path,\n",
        "    batch_size,\n",
        "    ratio = TEST_TRAIN_RATIO,\n",
        "    num_workers = 2,\n",
        "    pin_memory = True\n",
        "):\n",
        "\n",
        "  # Load dataset\n",
        "  dataset = DatasetFromPath(path)\n",
        "\n",
        "  # Split into training and testing dataset\n",
        "  # TO DO: Include Transformers\n",
        "  train, test = test_train_split(dataset, ratio=TEST_TRAIN_RATIO)\n",
        "  print(f\"Train inputs: {train['inputs'].shape}\")\n",
        "  print(f\"Train labels: {train['labels'].shape}\")\n",
        "  print(f\"Test inputs: {test['inputs'].shape}\")\n",
        "  print(f\"Test labels: {test['labels'].shape}\")\n",
        "\n",
        "  # Initialize data loaders\n",
        "  train_loader = DataLoader(\n",
        "      train,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      pin_memory = pin_memory,\n",
        "      shuffle=True)\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      test,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      pin_memory = pin_memory,\n",
        "      shuffle=True)\n",
        "\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "6quqjztmg-VV"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_MJ4kW1pEhwP",
        "qKs6HuxOzjMl",
        "wXT0jjZXA0Sl",
        "P8tk45vuaGsj",
        "IkoNQYudaMUl",
        "Gl7W7v-haPJO",
        "fuHHM5pXQ_DJ",
        "2rwTshMtruzG",
        "_I2Y9u71sEgT",
        "W3QlLRJCrjqP",
        "p-rwc6vWjZUR",
        "LBNZz_hzXhiJ",
        "X5L4ciSukV4y",
        "k73nwrXBkP-9",
        "NMM5PBYZXQb5"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}